from openpyxl import Workbook
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime
from bs4 import BeautifulSoup
import requests
from app import Class, db
from app import app
from urllib import parse
import json
import asyncio

def get_class_info(row,department):
    """
    æˆæ¥­ã®æƒ…å ±ã‚’å–å¾—ã™ã‚‹
    """
    error=""
    soup=BeautifulSoup(str(row),"html.parser")
    # æˆæ¥­ã‚³ãƒ¼ãƒ‰
    code=""
    
    code=soup.select_one("li  div.subjectListSummary > p:nth-child(2)").text.strip()
    try:
        code=code[code.find("ï¼š")+1:]
    except:
        code="å–å¾—ä¸å¯"
        error+=f"æˆæ¥­ã‚³ãƒ¼ãƒ‰å–å¾—ä¸å¯({soup.select_one("li  div.subjectListSummary > p:nth-child(2)").text.strip()})"
    # æˆæ¥­å
    name=soup.select_one("li h3 > span.jp").text.strip()
    # é–‹è¬›æ™‚æœŸ
    season=soup.select_one("li  div.subjectListSummary > p:nth-child(4)").text.strip()
    season=season[season.find("ï¼š")+1:]
    day=""
    time=-1
    if soup.select_one("li  div.subjectListSummary > p:nth-child(5)").text.strip().split("ï¼š")[1].find("é›†ä¸­ãƒ»ãã®ä»–")!=-1:
        day="é›†ä¸­ãƒ»ãã®ä»–"
        time=-10
    else:
        # æ›œæ—¥
        day=soup.select_one("li div.subjectListSummary > p:nth-child(5)").text.strip()
        try:
            day=day[day.find("ï¼š")+1:][0]
            time=int(soup.select_one("li  div.subjectListSummary > p:nth-child(5)").text.strip().split("ï¼š")[1][1])
        except:
            day="å–å¾—ä¸å¯"
            error+=f"æ›œæ—¥å–å¾—ä¸å¯({soup.select_one("li div.subjectListSummary > p:nth-child(5)").text.strip()})"
            time=-1
            error+=f"æ™‚é™å–å¾—ä¸å¯({soup.select_one("li  div.subjectListSummary > p:nth-child(5)").text.strip()})"
    # æ•™å“¡å
    teacher=""
    try:
        teacher=soup.select_one("li  h4 > span.jp").text.strip()
        teacher=teacher[teacher.find("ï¼š")+1:]
    except:
        teacher="å–å¾—ä¸å¯"
        error+=f"æ•™å“¡åå–å¾—ä¸å¯({soup.select_one("li  h4 > span.jp").text.strip()})"
    # æ•™å®¤å
    place=""
    try:
        place=soup.select_one("li  div.subjectListSummary > p:nth-child(6)").text.strip()
        place=place[place.find("ï¼š")+1:]
    except:
        place="å–å¾—ä¸å¯"
        error+=f"æ•™å®¤åå–å¾—ä¸å¯({soup.select_one("li  div.subjectListSummary > p:nth-child(6)").text.strip()})"
    # å˜ä½æ•°
    unit=-1
    try:
        unit=soup.select_one("li  div.subjectListSummary > p:nth-child(8)").text.strip()
        unit=int(unit[unit.find("ï¼š")+1:])
    except:
        unit=-1
        error+=f"å˜ä½æ•°å–å¾—ä¸å¯({soup.select_one("li  div.subjectListSummary > p:nth-child(8)").text.strip()})"
    note=""
    note=soup.select_one("li div.subjectListSummary > p:nth-child(9)").text.strip()
    try:
        note=note[note.find("ï¼š")+1:]
    except:
        error+=f"å‚™è€ƒå–å¾—ä¸å¯({soup.select_one("li div.subjectListSummary > p:nth-child(9)").text.strip()})"
    # é…å½“å¹´æ¬¡
    grade_min=-1
    grade_max=-1
    s=""
    try:
        s=soup.select_one("li div.subjectListSummary > p:nth-child(7)").text.strip()
        if s.find("ï½")==-1 and s.find("ãƒ»")==-1:
            grade_min=int(s.split("ï¼š")[1])
            grade_max=int(s.split("ï¼š")[1])
        else:
            s=s.replace("ï¼ˆ","").replace("ï¼‰","")
            grade_min=int(s.split("ï¼š")[1][0])
            grade_max=int(s.split("ï¼š")[1][2])
    except:
        grade_min=-1
        grade_max=-1
        error+=f"é…å½“å¹´æ¬¡å–å¾—ä¸å¯({s}) "
    # ã‚·ãƒ©ãƒã‚¹URL
    url=""
    try:
        url="https://syllabus.hosei.ac.jp/web/"+soup.select_one("li > a").attrs["href"]
    except:
        url="urlãªã—"
    _class=Class(department=department,year=2025,code=code,name=name,season=season,time=time,place=place,url=url,teacher=teacher,unit=unit,grade_min=grade_min,grade_max=grade_max,note=note,day=day,error=error)
    print(name,code,day,time,place,unit,grade_min,grade_max,note,error,sep="ğŸ‘¹")
    return _class
    


def get_class_list(department,page):
    """
    æˆæ¥­ã®ãƒªã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹
    """
    print(f"{department}ã®{page}ãƒšãƒ¼ã‚¸ç›®ã‚’å–å¾—ä¸­")
    url=r"https://syllabus.hosei.ac.jp/web/web_search_show.php?search=show&nendo=2025&gakubu="+parse.quote(department)+"&page="+str(page)
    # ã‚¹ãƒãƒ›ç‰ˆã‚µã‚¤ãƒˆã‚’ãƒªã‚¯ã‚¨ã‚¹ãƒˆã™ã‚‹
    headers = {
        "User-Agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 11_0 like Mac OS X) > > AppleWebKit/604.1.38 (KHTML, like Gecko) Version/11.0 Mobile/15A372 Safari/604.1",
        }
    response=requests.get(url,headers=headers)
    soup=BeautifulSoup(response.text,"html.parser")
    rows=soup.select("li.jp")
    return list(map(lambda row:get_class_info(row,department),rows))

DEPARTMENTS = [
    "æ³•å­¦éƒ¨",
    "æ–‡å­¦éƒ¨",
    "çµŒæ¸ˆå­¦éƒ¨",
    "ç¤¾ä¼šå­¦éƒ¨",
    "çµŒå–¶å­¦éƒ¨",
    "å›½éš›æ–‡åŒ–å­¦éƒ¨",
    "äººé–“ç’°å¢ƒå­¦éƒ¨",
    "ç¾ä»£ç¦ç¥‰å­¦éƒ¨",
    "æƒ…å ±ç§‘å­¦éƒ¨",
    "ã‚­ãƒ£ãƒªã‚¢ãƒ‡ã‚¶ã‚¤ãƒ³å­¦éƒ¨",
    "ç†å·¥å­¦éƒ¨",
    "ç”Ÿå‘½ç§‘å­¦éƒ¨",
    "ã‚°ãƒ­ãƒ¼ãƒãƒ«æ•™é¤Šå­¦éƒ¨",
    "ã‚¹ãƒãƒ¼ãƒ„å¥åº·å­¦éƒ¨"
    ]

def main():
    wb=Workbook()
    ws=wb.active
    ws.title="æˆæ¥­ä¸€è¦§"
    ws.append(["å­¦éƒ¨","æˆæ¥­ã‚³ãƒ¼ãƒ‰","æˆæ¥­å","é–‹è¬›æ™‚æœŸ","æ›œæ—¥","æ™‚é™","æ•™å®¤å","å˜ä½æ•°","é…å½“å¹´æ¬¡_æœ€å°","é…å½“å¹´æ¬¡_æœ€å¤§","ã‚·ãƒ©ãƒã‚¹URL","æ•™å“¡å","å‚™è€ƒ","ã‚¨ãƒ©ãƒ¼"])
    for i in DEPARTMENTS:
        cnt=1
        while True:
            class_list=get_class_list(i,cnt)
            if len(class_list)==0:
                print(f"{i}çµ‚ã‚ã‚Š") 
                break
            cnt+=1
            for j in class_list:
                ws.append([j.department,j.code,j.name,j.season,j.day,j.time,j.place,j.unit,j.grade_min,j.grade_max,j.url,j.teacher,j.note,j.error])
    wb.save("æˆæ¥­ä¸€è¦§.xlsx")


if __name__ == "__main__":
    main()